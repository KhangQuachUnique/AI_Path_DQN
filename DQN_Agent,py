import torch

from torch_geometric.nn import GCNConv

device = "cuda"

class GNN(torch.nn.Module):
    def __init__(self, in_features, hidden_dim, out_features):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(in_features, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, out_features)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return(x)

# Load model GNN
gnn = GNN(in_features=3, hidden_dim=16, out_features=3).to(device)
gnn.load_state_dict(torch.load("gnn_model_Q1.pth"))  # Load trá»ng sá»‘ Ä‘Ã£ train
gnn.eval()

graph_data = torch.load("graph_data_Q1.pth", weights_only=False)
nodes = graph_data["nodes"]
edges = graph_data["edges"]

import numpy as np

class Environment:
    def __init__(self, nodes, edges):
        self.nodes = nodes
        self.edges = edges
        self.node_list = list(nodes.keys())
        self.node_map = {n: i for i, n in enumerate(self.node_list)}
        self.adj_list = {node: [] for node in self.node_list}

        for edge in edges:
            u, v, length = edge["u"], edge["v"], edge["length"]
            if u in self.adj_list and v in self.adj_list:
                self.adj_list[u].append((v, length))
                self.adj_list[v].append((u, length))

        edge_list = [(self.node_map[edge["u"]], self.node_map[edge["v"]]) for edge in edges if edge["u"] in self.node_map and edge["v"] in self.node_map]
        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous().to(device)
        self.edge_index = edge_index

        node_features = torch.stack([torch.tensor([self.nodes[n]['lat'], self.nodes[n]['lon'], self.nodes[n]['street_count']]) for n in self.node_list]).to(device)
        self.node_features = node_features

        with torch.no_grad():
            node_embeddings = gnn(node_features, edge_index).to(device)
            self.node_embeddings = node_embeddings

        self.start = None
        self.goal = None
        self.current = None
        self.visited = set()

    def reset(self):
        self.start, self.goal = np.random.choice(self.node_list, 2, replace=False)
        self.current = self.node_map[self.start]  # Map start node to its index
        self.goal = self.node_map[self.goal]      # Map goal node to its index
        return self.get_state()

    def get_state(self):
        start_embedding = self.node_embeddings[self.current].cpu().numpy()
        goal_embedding = self.node_embeddings[self.goal].cpu().numpy()
        return np.concatenate((start_embedding, goal_embedding), axis=0).astype(np.float32)

    def step(self, action):
        neighbors = self.adj_list[self.node_list[self.current]]
        if action >= len(neighbors):
            return self.get_state(), -10, False

        next_node, _ = neighbors[action]
        if next_node not in self.node_map:
            return self.get_state(), -10, False  # Invalid action

        # Calculate distances
        current_distance = np.linalg.norm(
            np.array([self.nodes[self.node_list[self.current]]["lat"], self.nodes[self.node_list[self.current]]["lon"]]) -
            np.array([self.nodes[self.node_list[self.goal]]["lat"], self.nodes[self.node_list[self.goal]]["lon"]])
        )
        next_distance = np.linalg.norm(
            np.array([self.nodes[next_node]["lat"], self.nodes[next_node]["lon"]]) -
            np.array([self.nodes[self.node_list[self.goal]]["lat"], self.nodes[self.node_list[self.goal]]["lon"]])
        )

        # Reward based on progress toward the goal
        reward = current_distance - next_distance
        self.current = self.node_map[next_node]
        done = (self.current == self.goal)
        if done:
            reward += 100  # Large reward for reaching the goal

        return self.get_state(), reward, done



import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class QNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(QNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

import random
from collections import deque

class DQNAgent:
    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001, gamma=0.95):
        self.model = QNN(input_dim, hidden_dim, output_dim).to("cuda")
        self.target_model = QNN(input_dim, hidden_dim, output_dim).to("cuda")
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon = 0.1
        self.epsilon_min = 0.1
        self.epsilon_decay = 0
        self.memory = deque(maxlen=10000)

    def choose_action(self, state):
        if random.random() < self.epsilon:
            return random.randint(0, 5)
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to("cuda")
        return torch.argmax(self.model(state)).item()

    def store_experience(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train(self, batch_size=64):
        if len(self.memory) < batch_size:
            return

        batch = random.sample(self.memory, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.tensor(states, dtype=torch.float32).to("cuda")
        actions = torch.tensor(actions, dtype=torch.long).to("cuda")
        rewards = torch.tensor(rewards, dtype=torch.float32).to("cuda")
        next_states = torch.tensor(next_states, dtype=torch.float32).to("cuda")
        dones = torch.tensor(dones, dtype=torch.float32).to("cuda")

        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        next_q_values = self.target_model(next_states).max(1)[0].detach()
        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values

        loss = F.mse_loss(q_values, target_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

    def save_model(self, path="dqn_model.pth"):
        torch.save({
            "model_state_dict": self.model.state_dict(),
            "target_model_state_dict": self.target_model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "epsilon": self.epsilon
        }, path)
        print(f"âœ… Model saved at {path}")

    def load_model(self, path="dqn_model.pth"):
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.target_model.load_state_dict(checkpoint["target_model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.epsilon = checkpoint["epsilon"]
        print(f"ðŸ”„ Model loaded from {path}")


env = Environment(nodes, edges)
agent = DQNAgent(input_dim=6, hidden_dim=128, output_dim=6)
agent.load_model("dqn_model_final.pth")
agent.epsilon = 1

episodes = 10000
reached_goal = 0
for episode in range(episodes):
    state = env.reset()
    total_reward = 0

    for step in range(200):
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)
        agent.store_experience(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward
        if done:
            reached_goal += 1
            break

    agent.train() 
    agent.epsilon = max(agent.epsilon - agent.epsilon_decay, agent.epsilon_min)

    if episode % 100 == 0:
        agent.update_target_model()
        reached_goal = 0
        print(f"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon}, Reached goal: {reached_goal}")

    if episode % 10000 == 0:
        agent.save_model(f"dqn_model_{episode}.pth")

agent.save_model("dqn_model_final.pth")
print("âœ… Training hoÃ n táº¥t! Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u.")



# agent.load_model("dqn_model_final.pth")  # Load mÃ´ hÃ¬nh Ä‘Ã£ train trÆ°á»›c Ä‘Ã³

# state = env.reset()
# done = False
# path = [env.start]

# while not done:
#     action = agent.choose_action(state)
#     next_state, reward, done = env.step(action)
#     state = next_state
#     path.append(env.current)

# print("ðŸŽ¯ AI Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c Ä‘Æ°á»ng Ä‘i!")
# print("Lá»™ trÃ¬nh:", path)

# import folium

# def visualize_path(graph, path):
#     m = folium.Map(location=[10.775, 106.700], zoom_start=15)
#     coords = [(graph.nodes[n]['lat'], graph.nodes[n]['lon']) for n in path]
    
#     for lat, lon in coords:
#         folium.CircleMarker(location=[lat, lon], radius=5, color='blue').add_to(m)
    
#     folium.PolyLine(coords, color="red", weight=3).add_to(m)
    
#     return m

# # VÃ­ dá»¥ gá»i hÃ m
# m = visualize_path(graph_data, path)
# m.save("path_map.html")